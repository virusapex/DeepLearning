{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jrI6q7RmWQam"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrI6q7RmWQam"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab3/solutions/RL_Solution.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/aamini/introtodeeplearning/blob/master/lab3/solutions/RL_Solution.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkd375upWYok"
      },
      "source": [
        "# Copyright 2020 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved.\n",
        "# \n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of 6.S191 must\n",
        "# reference:\n",
        "#\n",
        "# © MIT 6.S191: Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoXYKhfZMHiw"
      },
      "source": [
        "# Обучение с подкреплением\n",
        "\n",
        "Обучение с подкреплением (RL) - это подмножество машинного обучения, которое ставит проблемы обучения как взаимодействие между агентами и средой. Часто предполагается, что агенты не имеют предварительных знаний о мире, поэтому они должны научиться ориентироваться в окружающей среде, оптимизируя функцию вознаграждения. В среде агент может предпринимать определенные действия и получать обратную связь в виде положительного или отрицательного вознаграждения за свое решение. Таким образом, цикл обратной связи агента напоминает идею \"проб и ошибок\", или то, как ребенок может научиться различать \"хорошие\" и \"плохие\" действия.\n",
        "\n",
        "С практической точки зрения, наш агент RL будет взаимодействовать с окружающей средой, выполняя действие на каждом временном шаге, получая соответствующее вознаграждение и обновляя свое состояние в соответствии с тем, чему он \"научился\".  \n",
        "\n",
        "![alt text](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
        "\n",
        "Хотя конечной целью обучения с подкреплением является обучение агентов действиям в реальном, физическом мире, игры представляют собой удобный полигон для разработки алгоритмов и агентов RL. Игры обладают некоторыми свойствами, которые делают их особенно подходящими для RL: \n",
        "\n",
        "1.   Во многих случаях игры имеют идеально описываемое окружение. Например, все правила игры в шахматы могут быть формально записаны и запрограммированы в симуляторе шахматной игры;\n",
        "2.   Игры поддаются массовому распараллеливанию. Поскольку они не требуют выполнения в реальном мире, одновременные среды можно запускать на больших кластерах данных; \n",
        "3.   Более простые сценарии в играх позволяют быстро создавать прототипы. Это ускоряет разработку алгоритмов, которые в конечном итоге могут работать в реальном мире; и\n",
        "4. ... Игры - это весело! \n",
        "\n",
        "Обучение с подкреплением принципиально отличается тем, что мы обучаем алгоритм глубокого обучения для управления действиями нашего агента RL, который пытается в своем окружении найти оптимальный способ достижения цели. Цель обучения RL-агента - определить наилучший следующий шаг, который нужно сделать, чтобы получить наибольшую конечную прибыль или доход. В этой лабораторной работе мы сосредоточимся на создании алгоритма обучения с подкреплением для освоения двух различных сред с разной сложностью. \n",
        "\n",
        "**Тележка с маятником**:   Уравновесить шест, выступающий из тележки, в вертикальном положении, перемещая основание только влево или вправо.\n",
        "\n",
        "Давайте начнем! Сначала мы импортируем TensorFlow, пакет курса и некоторые зависимости.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvdePP-VyVWp"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import base64, io, time, gym\n",
        "import IPython, functools\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmrHSiXKTXTY"
      },
      "source": [
        "Прежде чем мы погрузимся в процесс, давайте сделаем шаг назад и опишем наш подход, который в целом применим к проблемам обучения с усилением в целом:\n",
        "\n",
        "1. **Инициализируем среду и агента**: здесь мы опишем различные наблюдения и действия, которые агент может выполнять в среде.\n",
        "2. **Определить память нашего агента**: это позволит агенту помнить свои прошлые действия, наблюдения и вознаграждения.\n",
        "3. **Определите функцию вознаграждения**: описывает вознаграждение, связанное с действием или последовательностью действий.\n",
        "4. **Определите алгоритм обучения**: он будет использоваться для подкрепления хорошего поведения агента и предотвращения плохого поведения.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT7YL8KBJIIc"
      },
      "source": [
        "# Тележка с маятником\n",
        "\n",
        "## 1.1 Определение среды и агента Cartpole\n",
        "\n",
        "### Окружение \n",
        "\n",
        "Для моделирования среды для задач Cartpole и Pong мы будем использовать инструментарий, разработанный OpenAI под названием [OpenAI Gym] (https://gym.openai.com/). Он предоставляет несколько предустановленных сред для обучения и тестирования агентов обучения с подкреплением, включая среды для классических задач управления физикой, видеоигр Atari и симуляторов роботов. Для доступа к среде Cartpole мы можем использовать `env = gym.make(\"CartPole-v0\")`, доступ к которой мы получили при импорте пакета `gym`. Мы можем создавать различные [окружения](https://gym.openai.com/envs/#classic_control), передавая имя окружения в функцию `make`.\n",
        "\n",
        "Одна из проблем, с которой мы можем столкнуться при разработке алгоритмов RL, заключается в том, что многие аспекты процесса обучения по своей природе случайны: инициализация состояний игры, изменения в окружающей среде и действия агента. Поэтому для обеспечения некоторого уровня воспроизводимости может быть полезно задать начальную \"затравку\" для среды. Подобно тому, как вы можете использовать `numpy.random.seed`, мы можем вызвать сравнимую функцию в gym, `seed`, с нашей определенной средой, чтобы гарантировать, что случайные переменные среды инициализируются одинаково каждый раз."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quv9SC0iIYFm"
      },
      "source": [
        "### Instantiate the Cartpole environment ###\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "env.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhEITUcKK455"
      },
      "source": [
        "В игре Cartpole шест крепится неактивируемым шарниром к тележке, которая движется по дорожке без трения. Столб стоит вертикально, и задача состоит в том, чтобы не дать ему упасть. Система управляется путем приложения к тележке силы +1 или -1. За каждый временной интервал, в течение которого столб остается в вертикальном положении, выдается вознаграждение +1. Эпизод заканчивается, когда шест отклоняется от вертикали более чем на 15 градусов или тележка перемещается более чем на 2,4 единицы от центра дорожки. Визуальная схема окружения полюса тележки показана ниже:\n",
        "\n",
        "<img width=\"400px\" src=\"https://danielpiedrahita.files.wordpress.com/2017/02/cart-pole.png\"></img>.\n",
        "\n",
        "Учитывая эту установку для окружения и цель игры, мы можем подумать о том: 1) какие наблюдения помогают определить состояние среды; 2) какие действия может предпринять агент. \n",
        "\n",
        "Во-первых, давайте рассмотрим пространство наблюдений. В этой среде, нашими наблюдениями являются:\n",
        "\n",
        "1. Положение тележки\n",
        "2. Скорость тележки\n",
        "3. Угол поворота шеста\n",
        "4. Скорость вращения шеста\n",
        "\n",
        "Мы можем подтвердить размер пространства, запросив область видимости текущего пространства:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVJaEcbdIX82"
      },
      "source": [
        "n_observations = env.observation_space\n",
        "print(\"Environment has observation space =\", n_observations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZibGgjrALgPM"
      },
      "source": [
        "Во-вторых, мы рассматриваем пространство действий. На каждом временном шаге агент может двигаться либо вправо, либо влево. И снова мы можем подтвердить размер пространства действий путем опроса окружающей среды:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc9SIPxBIXrm"
      },
      "source": [
        "n_actions = env.action_space.n\n",
        "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPfHME8aRKkb"
      },
      "source": [
        "### Агент Cartpole\n",
        "\n",
        "Теперь, когда мы инстанцировали среду и поняли размерность пространств наблюдений и действий, мы готовы определить нашего агента. В глубоком обучении с подкреплением агента определяет глубокая нейронная сеть. Эта сеть принимает на вход наблюдение среды и выдает вероятность выполнения каждого из возможных действий. Поскольку Cartpole определяется низкоразмерным пространством наблюдений, для нашего агента должна хорошо подойти простая нейронная сеть с обратной связью. Мы определим ее с помощью API `Sequential`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-o_XK4oQ4eu"
      },
      "source": [
        "### Define the Cartpole agent ###\n",
        "\n",
        "# Defines a feed-forward neural network\n",
        "def create_cartpole_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      # First Dense layer\n",
        "      tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "\n",
        "      # TODO: Define the last Dense layer, which will provide the network's output.\n",
        "      # Think about the space the agent needs to act in!\n",
        "      tf.keras.layers.Dense(units=n_actions, activation=None) # TODO\n",
        "      # [TODO Dense layer to output action probabilities]\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "cartpole_model = create_cartpole_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5D5NSIYS2IW"
      },
      "source": [
        "Теперь, когда мы определили основную архитектуру сети, мы определим *функцию действия*, которая выполняет прямой проход по сети, учитывая набор наблюдений, и делает выборку из выходных данных. Эта выборка из выходных вероятностей будет использоваться для выбора следующего действия для агента. \n",
        "\n",
        "**Критически важно, что эта функция действия является абсолютно общей - мы будем использовать эту функцию как для Cartpole, и она также применима к другим задачам RL!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_vVZRr8Q4R_"
      },
      "source": [
        "### Define the agent's action function ###\n",
        "\n",
        "# Function that takes observations as input, executes a forward pass through model, \n",
        "#   and outputs a sampled action.\n",
        "# Arguments:\n",
        "#   model: the network that defines our agent\n",
        "#   observation: observation which is fed as input to the model\n",
        "# Returns:\n",
        "#   action: choice of agent action\n",
        "def choose_action(model, observation):\n",
        "  # add batch dimension to the observation\n",
        "  observation = np.expand_dims(observation, axis=0)\n",
        "\n",
        "  '''TODO: feed the observations through the model to predict the log probabilities of each possible action.'''\n",
        "  logits = model.predict(observation) # TODO\n",
        "  # logits = model.predict('''TODO''')\n",
        "  \n",
        "  # pass the log probabilities through a softmax to compute true probabilities\n",
        "  prob_weights = tf.nn.softmax(logits).numpy()\n",
        "  \n",
        "  '''TODO: randomly sample from the prob_weights to pick an action.\n",
        "  Hint: carefully consider the dimensionality of the input probabilities (vector) and the output action (scalar)'''\n",
        "  action = np.random.choice(n_actions, size=1, p=prob_weights.flatten())[0] # TODO\n",
        "  # action = np.random.choice('''TODO''', size=1, p=''''TODO''')['''TODO''']\n",
        "\n",
        "  return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tR9uAWcTnkr"
      },
      "source": [
        "## 1.2 Определение памяти агента\n",
        "\n",
        "Теперь, когда мы инстанцировали среду и определили архитектуру сети агента и функцию действия, мы готовы перейти к следующему шагу в нашем рабочем процессе RL:\n",
        "1. **Инициализация среды и агента**: здесь мы опишем различные наблюдения и действия, которые агент может выполнять в среде.\n",
        "2. **Определите память нашего агента**: это позволит агенту помнить свои прошлые действия, наблюдения и награды.\n",
        "3. **Определите алгоритм обучения**: он будет использоваться для подкрепления хорошего поведения агента и предотвращения плохого поведения.\n",
        "\n",
        "В обучении с подкреплением обучение происходит параллельно с действиями агента в окружающей среде; под *эпизодом* понимается последовательность действий, которая заканчивается некоторым конечным состоянием, например, падением столба или падением тележки. Агент должен запомнить все свои наблюдения и действия, чтобы после завершения эпизода научиться \"подкреплять\" хорошие действия и наказывать нежелательные действия посредством обучения. Наш первый шаг - определить простой буфер памяти, который содержит наблюдения, действия и полученные вознаграждения агента за данный эпизод. \n",
        "\n",
        "**Опять же, обратите внимание на модульность этого буфера памяти - он может и будет применяться и для других задач RL!**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MM6JwXVQ4JG"
      },
      "source": [
        "### Agent Memory ###\n",
        "\n",
        "class Memory:\n",
        "  def __init__(self): \n",
        "      self.clear()\n",
        "\n",
        "  # Resets/restarts the memory buffer\n",
        "  def clear(self): \n",
        "      self.observations = []\n",
        "      self.actions = []\n",
        "      self.rewards = []\n",
        "\n",
        "  # Add observations, actions, rewards to memory\n",
        "  def add_to_memory(self, new_observation, new_action, new_reward): \n",
        "      self.observations.append(new_observation)\n",
        "      '''TODO: update the list of actions with new action'''\n",
        "      self.actions.append(new_action) # TODO\n",
        "      # ['''TODO''']\n",
        "      '''TODO: update the list of rewards with new reward'''\n",
        "      self.rewards.append(new_reward) # TODO\n",
        "      # ['''TODO''']\n",
        "        \n",
        "memory = Memory()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YhtPaUVj5m"
      },
      "source": [
        "## 1.3 Функция вознаграждения\n",
        "\n",
        "Мы почти готовы начать алгоритм обучения нашего агента! Следующим шагом будет вычисление вознаграждения нашего агента в процессе его действий в окружающей среде. Поскольку мы (и агент) не знаем, закончится ли игра или задание (т.е. когда упадет столб) и когда это произойдет, полезно подчеркнуть получение вознаграждения **сейчас**, а не позже в будущем - в этом заключается идея дисконтирования (**Дисконтирование — определение стоимости денежного потока путём приведения стоимости всех выплат к определённому моменту времени.**). Это понятие схоже с дисконтированием денег в случае с процентами. Как вы помните из лекции, мы используем дисконтирование вознаграждения, чтобы отдать большее предпочтение получению вознаграждения сейчас, а не позже в будущем. Идея дисконтирования вознаграждения похожа на дисконтирование денег в случае с процентами.\n",
        "\n",
        "Чтобы вычислить ожидаемое кумулятивное вознаграждение, известное как **доходность**, на данном временном шаге в эпизоде обучения, мы суммируем дисконтированные вознаграждения, ожидаемые на данном временном шаге $t$, в рамках эпизода обучения и проецируя их в будущее. Мы определяем доходность (кумулятивное вознаграждение) на временном шаге $t$, $R_{t}$ как:\n",
        "\n",
        ">$R_{t}=\\sum_{k=0}^\\infty\\gamma^kr_{t+k}$\n",
        "\n",
        "где $0 < \\gamma < 1$ - коэффициент дисконтирования, $r_{t}$ - вознаграждение на временном шаге $t$, а индекс $k$ увеличивает проекцию в будущее в рамках одного эпизода обучения. Интуитивно можно представить, что эта функция обесценивает любые вознаграждения, полученные на более поздних временных шагах, что заставляет агента отдавать приоритет получению вознаграждения сейчас. Поскольку мы не можем расширять эпизоды до бесконечности, на практике вычисления будут ограничены количеством временных шагов в эпизоде - после этого вознаграждение принимается равным нулю.\n",
        "\n",
        "Обратите внимание на форму этой суммы - нам придется проявить смекалку при реализации этой функции. В частности, нам нужно будет инициализировать массив нулей длиной в количество временных шагов и заполнять его реальными значениями дисконтированного вознаграждения по мере того, как мы будем перебирать вознаграждения из эпизода, которые будут сохранены в памяти агента. В конечном итоге нас интересует, какие действия лучше по сравнению с другими действиями, предпринятыми в этом эпизоде, поэтому мы нормализуем наши вычисленные вознаграждения, используя среднее значение и стандартное отклонение вознаграждений по всему учебному эпизоду.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Q2OFYtQ32X"
      },
      "source": [
        "### Reward function ###\n",
        "\n",
        "# Helper function that normalizes an np.array x\n",
        "def normalize(x):\n",
        "  x -= np.mean(x)\n",
        "  x /= np.std(x)\n",
        "  return x.astype(np.float32)\n",
        "\n",
        "# Compute normalized, discounted, cumulative rewards (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma=0.95): \n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  for t in reversed(range(0, len(rewards))):\n",
        "      # update the total discounted reward\n",
        "      R = R * gamma + rewards[t]\n",
        "      discounted_rewards[t] = R\n",
        "      \n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzbY-mjGYcmt"
      },
      "source": [
        "## 1.4 Алгоритм обучения\n",
        "\n",
        "Теперь мы можем начать определять алгоритм обучения, который будет использоваться для подкрепления хорошего поведения агента и препятствования плохому поведению. В этой лабораторной работе мы сосредоточимся на методах *policy gradient*, которые направлены на **максимизацию** вероятности действий, приводящих к большим вознаграждениям. Эквивалентно это означает, что мы хотим **минимизировать** отрицательную вероятность этих же действий. Мы достигаем этого, просто **масштабируя** вероятности на соответствующее вознаграждение - эффективно усиливая вероятность действий, которые приводят к большим вознаграждениям.\n",
        "\n",
        "Поскольку функция log монотонно возрастает, это означает, что минимизация **отрицательной вероятности** эквивалентна минимизации **отрицательной log-вероятности**.  Напомним, что мы можем легко вычислить отрицательное лог-вероятность дискретного действия, оценив его [softmax cross entropy](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits). Как и в контролируемом обучении, мы можем использовать методы стохастического градиентного спуска для достижения желаемой минимизации. \n",
        "\n",
        "Начнем с определения функции потерь."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsgZ3IDCY_Zn"
      },
      "source": [
        "### Loss function ###\n",
        "\n",
        "# Arguments:\n",
        "#   logits: network's predictions for actions to take\n",
        "#   actions: the actions the agent took in an episode\n",
        "#   rewards: the rewards the agent received in an episode\n",
        "# Returns:\n",
        "#   loss\n",
        "def compute_loss(logits, actions, rewards): \n",
        "  '''TODO: complete the function call to compute the negative log probabilities'''\n",
        "  neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions) # TODO\n",
        "  # neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits='''TODO''', labels='''TODO''')\n",
        "  \n",
        "  '''TODO: scale the negative log probability by the rewards'''\n",
        "  loss = tf.reduce_mean( neg_logprob * rewards ) # TODO\n",
        "  # loss = tf.reduce_mean('''TODO''')\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr5vQ9fqbPpp"
      },
      "source": [
        "Теперь давайте используем функцию потерь для определения шага обучения нашего алгоритма обучения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_50ada7nbZ7L"
      },
      "source": [
        "### Training step (forward and backpropagation) ###\n",
        "\n",
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "  with tf.GradientTape() as tape:\n",
        "      # Forward propagate through the agent network\n",
        "      logits = model(observations)\n",
        "\n",
        "      '''TODO: call the compute_loss function to compute the loss'''\n",
        "      loss = compute_loss(logits, actions, discounted_rewards) # TODO\n",
        "      # loss = compute_loss('''TODO''', '''TODO''', '''TODO''')\n",
        "\n",
        "  '''TODO: run backpropagation to minimize the loss using the tape.gradient method'''\n",
        "  grads = tape.gradient(loss, model.trainable_variables) # TODO\n",
        "  # grads = tape.gradient('''TODO''', model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsjKXh6BcgjR"
      },
      "source": [
        "## 1.5 Запустите Cartpole!\n",
        "\n",
        "Не имея никаких предварительных знаний об окружающей среде, агент начнет учиться балансировать шест на тележке, основываясь только на обратной связи, получаемой из окружающей среды! Определив, как наш агент может двигаться, как он принимает новые наблюдения и как он обновляет свое состояние, мы увидим, как он постепенно учится политике действий для оптимизации балансировки шеста как можно дольше. Для этого мы проследим, как вознаграждение изменяется в зависимости от обучения - как должно меняться вознаграждение по мере обучения?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmOzc2rrcn8Q"
      },
      "source": [
        "### Cartpole training! ###\n",
        "\n",
        "# Learning rate and optimizer\n",
        "learning_rate = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# instantiate cartpole agent\n",
        "cartpole_model = create_cartpole_model()\n",
        "\n",
        "# to track our progress\n",
        "smoothed_reward = mdl.util.LossHistory(smoothing_factor=0.9)\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Rewards')\n",
        "\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "for i_episode in range(200):\n",
        "\n",
        "  plotter.plot(smoothed_reward.get())\n",
        "\n",
        "  # Restart the environment\n",
        "  observation = env.reset()\n",
        "  memory.clear()\n",
        "\n",
        "  while True:\n",
        "      # using our observation, choose an action and take it in the environment\n",
        "      action = choose_action(cartpole_model, observation)\n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "      # add to memory\n",
        "      memory.add_to_memory(observation, action, reward)\n",
        "      \n",
        "      # is the episode over? did you crash or do so well that you're done?\n",
        "      if done:\n",
        "          # determine total reward and keep a record of this\n",
        "          total_reward = sum(memory.rewards)\n",
        "          smoothed_reward.append(total_reward)\n",
        "          \n",
        "          # initiate training - remember we don't know anything about how the \n",
        "          #   agent is doing until it has crashed!\n",
        "          train_step(cartpole_model, optimizer, \n",
        "                     observations=np.vstack(memory.observations),\n",
        "                     actions=np.array(memory.actions),\n",
        "                     discounted_rewards = discount_rewards(memory.rewards))\n",
        "          \n",
        "          # reset the memory\n",
        "          memory.clear()\n",
        "          break\n",
        "      # update our observatons\n",
        "      observation = next_observation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkcUtGF1VE-K"
      },
      "source": [
        "Чтобы получить представление о работе нашего агента, мы можем сохранить видео, на котором обученная модель работает над балансировкой шеста. Поймите, что это совершенно новая среда, которую агент раньше не видел!\n",
        "\n",
        "Давайте покажем сохраненное видео, чтобы посмотреть, как наш агент справился с задачей!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAYBkv6Zbk0J"
      },
      "source": [
        "saved_cartpole = mdl.lab3.save_video_of_model(cartpole_model, \"CartPole-v0\")\n",
        "mdl.lab3.play_video(saved_cartpole)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSbVNDpaVb3_"
      },
      "source": [
        "Как работает агент? Могли бы вы тренировать его в течение более короткого времени и при этом показывать хорошие результаты? Думаете ли вы, что более длительная тренировка поможет еще больше? "
      ]
    }
  ]
}